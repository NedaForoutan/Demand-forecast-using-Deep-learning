# -*- coding: utf-8 -*-
"""retail

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TpIRst7XNcux6AWtp8KSLT5fIzoGwm4A
"""

import torch
import torch.nn as nn

import seaborn as sn
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler

import scipy.stats as stats
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error
from datetime import datetime
import math
# Importing the most popular regression libraries.
from sklearn.linear_model import LinearRegression 
from sklearn.neighbors import KNeighborsRegressor
from sklearn.linear_model import Ridge
from sklearn.linear_model import Lasso
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import ExtraTreesRegressor
from xgboost import XGBRegressor

# import os
# print(os.listdir("../input"))
#pd.read_csv("../input/sales data-set.csv")

#loading data
feature = pd.read_csv('datasets_2296_3883_Features data set.csv')
store = pd.read_csv('datasets_2296_3883_stores data-set.csv')
sales = pd.read_csv('sales data-set.csv')

#merge all files in one data frame
data = sales.merge(feature, how = 'left', on=['Store', 'Date', 'IsHoliday'])
data = data.merge(store,how= "left", on=['Store'])
#print(len(data.columns))
#print(data.shape)

num_store = store.Type.value_counts()
print(num_store) # A has highest number of stores

holiday = data.IsHoliday.value_counts()
print(holiday)

#preprocessing
data=data.fillna(0)
#data.isna().sum()  #check for any NaN

print((data.Weekly_Sales<0).any())  #negative value in weekly sales
#ignore negative ones
data = data[data.Weekly_Sales>0]
print((data.Weekly_Sales<0).any())
print(data.shape)

#is there ant duplicate row
print(data.duplicated().sum())

#IsHoliday 0,1
data.IsHoliday = data.IsHoliday.replace({True: 1, False: 0})

#Type 1,2,3
data.Type = data.Type.replace({"A": 1, "B": 2, "C": 3})

print(data[:5])

#Normalizing
#def normalize():
scalar = MinMaxScaler(feature_range=(0, 1))
norm_columns = ['Temperature', 'Fuel_Price', 'CPI', 'Unemployment', 'Size','MarkDown1','MarkDown2','MarkDown3','MarkDown4','MarkDown5']
for col in norm_columns:
  data[col] = scalar.fit_transform(np.array(data[col]).reshape(-1,1))

data[:5]

print(data[0:1].IsHoliday.item())

date = data.Date.unique()
store = data.Store.unique()
new = pd.DataFrame(columns=data.columns)
new = new.drop(['Dept'], axis = 1)


for d in date:
  for s in store:
    temp = data[data.Store == s]
    temp = temp[temp.Date == d]
    new = new.append({'Store': s, 'Date':d, 'Weekly_Sales' : temp.Weekly_Sales.mean(), 'IsHoliday': temp[0:1].IsHoliday.item(), 'Temperature':temp[0:1].Temperature.item(), 'Fuel_Price' : temp[0:1].Fuel_Price.item(), 'MarkDown1': temp[0:1].MarkDown1.mean(),	'MarkDown2': temp[0:1].MarkDown2.mean(), 'MarkDown3': temp[0:1].MarkDown3.mean(), 'MarkDown4' : temp[0:1].MarkDown4.mean(),	'MarkDown5' : temp[0:1].MarkDown5.mean(),	'CPI': temp[0:1].CPI.mean(),	'Unemployment': temp[0:1].Unemployment.item(),	'Type' : temp[0:1].Type.item() , 'Size': temp[0:1].Size.mean() }, ignore_index=True)

#print(new[:5])

data = new

#correlation matrix
cor_matrix = data.corr()
cor_matrix

#As we see MarkDown 4 and MarkDown1 are high correlated
f, ax = plt.subplots(figsize=(12, 9))
sn.heatmap(cor_matrix, vmax=.8, square=True, annot=True)
#plt.show()

#Sorting data by "Date"
data['year'] = pd.to_datetime(data['Date']).dt.year
data['month'] = pd.to_datetime(data['Date']).dt.month
data['Date'] = pd.to_datetime(data['Date']).dt.date
data.sort_values(by='Date', inplace=True, ascending=True)

train = data[data.year.isin([2010, 2011])]

valid = data[(data.year == 2012 ) & (data.month.isin([1,2,3,4,5,6,7]))]

train = train.drop(['year', 'month'], axis = 1)
valid = valid.drop(['year', 'month'], axis = 1)
#print(train.shape)

test = data[data.year == 2012]
test = test[test.month.isin([8,9,10])]  #Aug - Oct

test = test.drop(['year', 'month'], axis = 1)

#print(test.shape)
data = data.drop(['year', 'month'], axis = 1)

#just normalizing y_train :Weekly_Sales
scalar = MinMaxScaler(feature_range=(0, 1))
train['Weekly_Sales'] = scalar.fit_transform(np.array(train.Weekly_Sales).reshape(-1,1))
valid['Weekly_Sales'] = scalar.fit_transform(np.array(valid.Weekly_Sales).reshape(-1,1))

#indexing data by 'Date'
train = train.set_index(['Date'], drop = True)
valid = valid .set_index(['Date'], drop = True)
test = test.set_index(['Date'], drop = True)

#creating window data set
def create_inout_sequences(input_data, tw):
    inout_seq = []
    L = len(input_data)
    for i in range(L-tw):
        train_seq = input_data[i:i+tw]
        train_label = input_data[:, 1:2][i+tw:i+tw+1]
        inout_seq.append((train_seq ,train_label))
    return inout_seq

num_week = 52  #365/7 number of week in a year
num_store = 45
window = num_week * num_store

#pass data to tensor 
train_tensor = torch.FloatTensor(train.values.astype(np.float32))

#convert our training data into sequences and corresponding labels
train_inout_seq = create_inout_sequences(train_tensor, window)

train_inout_seq[1]

#LSTM
class LSTM(nn.Module):
    def __init__(self, input_size=14, hidden_layer_size=100, output_size=num_store):
        super().__init__()
        self.hidden_layer_size = hidden_layer_size

        self.lstm = nn.LSTM(input_size, hidden_layer_size)

        self.linear = nn.Linear(hidden_layer_size, output_size)

        self.hidden_cell = (torch.zeros(1,1,self.hidden_layer_size),
                            torch.zeros(1,1,self.hidden_layer_size))

    def forward(self, input_seq):
        lstm_out, self.hidden_cell = self.lstm(input_seq.view(len(input_seq) ,1, -1), self.hidden_cell)
        predictions = self.linear(lstm_out.view(len(input_seq), -1))
        return predictions[-1]

#weighted holidays
def WMSE (X, Y, pred):
  weight = x[: , 2:3].apply(lambda holiday:5 if holiday else 1)
  return np.sum(weight * np.square(Y - pred), axis = 0) / np.sum(weight)

model = LSTM()
loss_function = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

print(model)

lr = 11e-3

#Train


epochs = 150
best_acc = 0
patience = 5
count = 0

for i in range(epochs):
    for seq, labels in train_inout_seq:
        optimizer.zero_grad()
        model.hidden_cell = (torch.zeros(1, 1, model.hidden_layer_size),
                        torch.zeros(1, 1, model.hidden_layer_size))

        y_pred = model(seq)

        
        single_loss = loss_function(y_pred, labels) #WMSE(seq, labels, y_pred)
        print(single_loss.item())
        single_loss.backward()
        optimizer.step()

    if i%25 == 1:
        print(f'epoch: {i:3} loss: {single_loss.item():10.8f}')

    # Code to update the lr
    lr *= learning_rate_decay
    update_lr(optimizer, lr)
    with torch.no_grad():
        correct = 0
        total = 0
        for seq, labels in train_inout_seq:
          y_pred = model(seq)
          _, predicted = torch.max(y_pred.data, 1)
          total += labels.size(0)
          correct += (predicted == labels).sum().item()

        #Early stopping to get best model from validation  
        val_acc = 100 * correct /total
        if val_acc > best_acc:
          best_acc = val_acc
          best_model = model
          torch.save(model.state_dict(), 'checkpoint.pt')
          count = 0
        else:   #after how many epoches not improving of validation accuracy stop
          count += 1
          if count >= patience:
            break;

        print('Validataion accuracy is: {} %'.format(100 * correct / total))

print(f'epoch: {i:3} loss: {single_loss.item():10.10f}')

best_model = torch.load('checkpoint.pt')
model.load_state_dict(best_model)

model.eval()

for i in range(len(test)):
    seq = torch.FloatTensor(test_inputs[-window:])
    with torch.no_grad():
        model.hidden = (torch.zeros(1, 1, model.hidden_layer_size),
                        torch.zeros(1, 1, model.hidden_layer_size))
        test_inputs.append(model(seq).item())